---
layout: post
comments: true
title: Post Template
author: Lawrence Liao, John Li, Guanhua Ji, William Wu
date: 2024-03-22
---

> This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.

<!--more-->

{: class="table-of-content"}
* TOC
{:toc}

## Main Content

Your survey starts here. You can refer to the [source code](https://github.com/lilianweng/lil-log/tree/master/_posts) of [lil's blogs](https://lilianweng.github.io/lil-log/) for article structure ideas or Markdown syntax. We've provided a [sample post](https://ucladeepvision.github.io/CS188-Projects-2022Winter/2017/06/21/an-overview-of-deep-learning.html) from Lilian Weng and you can find the source code [here](https://raw.githubusercontent.com/UCLAdeepvision/CS188-Projects-2022Winter/main/_posts/2017-06-21-an-overview-of-deep-learning.md)

## Introduction

Our team is investigating style transfer in the context of image-to-image translation.

## Background

### Image-to-image translation

The goal of image-to-image translation is learning a mapping between input image and the output image. The dataset used for this problem is typically two sets of images we want to learn the mapping between. These datasets come in the form of

1. Paired: the dataset is tuples of image in set 1 and corresponding image in set 2
2. Unpaired: the dataset just has two sets of images without 1-to-1 correspondence.

Paired datasets are easier to train on, but they maybe hard to collect (e.g. there is not a Monet painitng for every real image). Thus, we need methods to effectively train on unpaired datasets.

## Cycle-GAN

Generative adversarial network (GAN) are deep learning frameworks that relies on a generator $$G$$ and a discriminator $$D$$. Cycle GAN introduces **cycle consistency** (similar to language translation, where a sentence in English when translated to German then translated back should be the same as English).

The goal of our Cycle GAN is to learn a mapping between two image styles $$X$$ and $$Y$$. So if we have preserve cycle consistency, the idea is that our tralated image will preserve most of its semanics besides the style change.

To preserve cycle consistency, we want to make sure when our network translates an image, we can translate it back to get a similar image to the original image. In order to do this, we train two GANs together, Gan 1 $$(G, D_Y)$$ translating from style $$X$$ to style $$Y$$. Gan 2 $$(F, D_X)$$ translating from style $$Y$$ to style $$X$$. We additionally introduce a normalization term on the input image $$I$$ and the $$F(G(I))$$, the input image translated twice.

Now, in the actual style transfer process, we can use $$G$$ to translate from style $$X$$ to style $$Y$$, and $$F$$ to translate from style $$Y$$ to style $$X$$.

### Training

To train Gan 1 $$(G, D_Y)$$, where $$G$$ is a mapping from $$X$$ to $$Y$$ and $$D_Y$$ is a discriminator for $$Y$$, we use the following loss function:

$$
\mathcal{L}_{\text{GAN}}(G, D_Y, X, Y) = \mathbb{E}_{y \sim p_{\text{data}}(y)}[\log D_Y(y)] + \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log(1 - D_Y(G(x)))]
$$

We want $$G(x)$$ to generate images similar to the distribution of $$Y$$, and we want $$D_Y$$ to distinguish between images generated by $$G$$ and images from $$Y$$. So we wan $$G$$ to minimize the loss and $$D_Y$$ to maximize the loss.

Additionally, to preserve cycle consistency, we need a cycle consistency loss:

$$
\mathcal{L}_{\text{cyc}}(G, F) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\|F(G(x)) - x\|_1] + \mathbb{E}_{y \sim p_{\text{data}}(y)}[\|G(F(y)) - y\|_1]
$$

The idea is that for $$x \in X$$ and $$y \in Y$$, we want $$F(G(x))$$ to be similar to $$x$$ and $$G(F(y))$$ to be similar to $$y$$.

The overall obejctive function is then:

$$
\mathcal{L}(G, F, D_X, D_Y) = \mathcal{L}(G, D_Y, X, Y) + \mathcal{L}(F, D_X, Y, X) + \lambda \mathcal{L}_{\text{cyc}}(G, F)
$$

where $$\lambda$$ is a hyperparameter that controls the importance of the cycle consistency loss. And we hope to find $$G^*, F^*$$ where

$$
G^*, F^* = \arg \min_{G, F} \max_{D_X, D_Y} \mathcal{L}(G, F, D_X, D_Y)
$$

## Stable Diffusion

Stable Diffusion is in the class of latent diffusion models (LDMs). Similar to diffusion models, LDMs work by repeatedly removing noise from an image, but instead of starting in the pixel space, LDMs start in the latent space.

Why might this be a good idea?  
We can reduce the dimensionality with minimal loss of information! This makes training and image generation more efficient and allows for high-resolution image synthesis.

### Diffusion Model

Before diving into Stable Diffusion, we start by discussing the fundamental diffusion model. The diffusion model is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time [1]. The process it divided into two parts: the forward process which adds noise to the original image, and the backward process which gradually denoise the noisy input to get an image.

![Diffusion]({{ '/assets/images/team26/diffusion.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. The diffusion and reverse illustrated as a directed graphical model* [1].

In the forward pass, we add noise to the input image following a variance schedule $$\beta_1, ..., \beta_T$$. The approximate posterior (_diffusion process_) is defined as:

$$
q(\mathbf{x}_{1:T}|\mathbf{x}_0) := \prod_{t=1}^T q(\mathbf{x}_t|\mathbf{x}_{t-1})
$$

where

$$
q(\mathbf{x}_t|\mathbf{x}_{t-1}) := \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t}\mathbf{x}_{t-1},\beta_t\mathbf{I})
$$

The quantity we are maximizing is the variational lower bound (VLB) as seen in VAE models.

The backward denoising process obtains a joint distribution (_reverse process_), which is defined as a Markov chain with learned Gaussian transitions starting at $$p(\mathbf{x}_T) = \mathcal{N}(\mathbf{x}_T;\mathbf{0},\mathbf{I})$$:

$$
p_{\theta}(\mathbf{x}_{0:T}) := p(\mathbf{x}_T) \prod_{t=1}^T p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t)
$$

where

$$
p_{\theta}(\mathbf{x}_{t-1}|\mathbf{x}_t) := \mathcal{N}(\mathbf{x}_{t-1}; \mathbf{\mu}_{\theta}(\mathbf{x}_t, t), \Sigma_{\theta}(\mathbf{x}_t, t))
$$

During the backward process, instead of directly denoising the image, predicting the noise and subtracting it from the image turned out to be a more feasible approach.

While the conventional diffusion model achieves satisfying results by generating high quality images, it directly operates in pixel space, making the training process expensive in terms of computational resources. The sequential evaluation for inference also made the model slow. In order to train on limited computational resources while sacrificing little loss in generated image quality and flexibility, we turn to the Latent Diffusion Model (LDM).

### Latend Diffusion Model

Instead of sampling from the pixel space, the Latent Diffusion Model samples from the latent space of a powerful pretrained autoencoder [2]. The autoencoder is universal, and is trained only once in order to apply multiple LDM trainings. The autoencoder is trained by combination of a perceptual loss and a patch-based adversarial objective [2] such that the reconstruction are confined to the image manifold and avoids blurriness if solely relying on pixel space losses. More formally, the 

### Training

[TODO]

## Image Captioning with Transformers

To caption an image, we can use a encoder-decoder architecture. To do so, we use a pre-trained Transformer-based vision model as encoder and a pre-trained language model as decoder. The encoder produces an embedding of the image, which can be used by the decoder to generate a caption.

## Reference

[1] Ho, J., Jain, A. N., & Abbeel, P. (2020). Denoising diffusion probabilistic models. _arXiv (Cornell University)_.

[2] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., & Ommer, B. (2022). High-Resolution Image Synthesis with Latent Diffusion Models. _2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)_.

---
