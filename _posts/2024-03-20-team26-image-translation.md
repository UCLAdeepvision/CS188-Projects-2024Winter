---
layout: post
comments: true
title: Post Template
author: UCLAdeepvision
date: 2024-03-22
---

> This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.

<!--more-->

{: class="table-of-content"}

- TOC
  {:toc}

## Main Content

Your survey starts here. You can refer to the [source code](https://github.com/lilianweng/lil-log/tree/master/_posts) of [lil's blogs](https://lilianweng.github.io/lil-log/) for article structure ideas or Markdown syntax. We've provided a [sample post](https://ucladeepvision.github.io/CS188-Projects-2022Winter/2017/06/21/an-overview-of-deep-learning.html) from Lilian Weng and you can find the source code [here](https://raw.githubusercontent.com/UCLAdeepvision/CS188-Projects-2022Winter/main/_posts/2017-06-21-an-overview-of-deep-learning.md)

## Introduction

Our team is investigating style transfer in the context of image-to-image translation.

## Background

### Image-to-image translation

The goal of image-to-image translation is learning a mapping between input image and the output image. The dataset used for this problem is typically two sets of images we want to learn the mapping between. These datasets come in the form of

1. Paired: the dataset is tuples of image in set 1 and corresponding image in set 2
2. Unpaired: the dataset just has two sets of images without 1-to-1 correspondence.

Paired datasets are easier to train on, but they maybe hard to collect (e.g. there is not a Monet painitng for every real image). Thus, we need methods to effectively train on unpaired datasets.

## Cycle-GAN

Generative adversarial network (GAN) are deep learning frameworks that relies on a generator G and a discriminator D. Cycle GAN introduces **cycle consistency** (similar to language translation, where a sentence in English when translated to German then translated back should be the same as English).

The goal of our Cycle GAN is to learn a mapping between two image styles $X$ and $Y$. So if we have preserve cycle consistency, the ideaaaa is that our tralated image will preserve most of its semanics besides the style change.

To preserve cycle consistency, we want to make sure when our network translates an image, we can translate it back to get a similar image to the original image. In order to do this, we train two GANs together, Gan 1 $(G, D_Y)$ translating from style $X$ to style $Y$. Gan 2 $(F, D_X)$ translating from style $Y$ to style $X$. We additionally introduce a normalization term on the input image $I$ and the $F(G(I))$, the input image translated twice.

Now, in the actual style transfer process, we can use $G$ to translate from style $X$ to style $Y$, and $F$ to translate from style $Y$ to style $X$.

### Training

To train Gan 1 $(G, D_Y)$, where $G$ is a mapping from $X$ to $Y$ and $D_Y$ is a discriminator for $Y$, we use the following loss function:

$$
\mathcal{L}_{\text{GAN}}(G, D_T, X, Y) = \mathbb{E}_{y\ sim p_{\text{data}}(y)}[\log D_Y(y)] + \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log(1 - D_Y(G(x)))]
$$

We want $G(x)$ to generate images similar to the distribution of $Y$, and we want $D_Y$ to distinguish between images generated by $G$ and images from $Y$. So we wan $G$ to minimize the loss and $D_Y$ to maximize the loss.

Additionally, to preserve cycle consistency, we need a cycle consistency loss:

$$
\mathcal{L}_{\text{cyc}}(G, F) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\|F(G(x)) - x\|_1] + \mathbb{E}_{y \sim p_{\text{data}}(y)}[\|G(F(y)) - y\|_1]
$$

The idea is that for $x \in X$ and $y \in Y$, we want $F(G(x))$ to be similar to $x$ and $G(F(y))$ to be similar to $y$.

The overall obejctive function is then:

$$
\mathcal{L}(G, F, D_X, D_Y) = \mathcal{L}(G, D_Y, X, Y) + \mathcal{L}(F, D_X, Y, X) + \lambda \mathcal{L}_{\text{cyc}}(G, F)
$$

where $\lambda$ is a hyperparameter that controls the importance of the cycle consistency loss. And we hope to find $G^*, F^*$ where

$$
G^*, F^* = \arg \min_{G, F} \max_{D_X, D_Y} \mathcal{L}(G, F, D_X, D_Y)
$$

## Stable Diffusion

Stable Diffusion is in the class of latent diffusion models (LDMs). Similar to diffusion models, LDMs work by repeatedly removing noise from an image, but instead of starting in the pixel space, LDMs start in the latent space. 

Why might this be a good idea?  
We can reduce the dimensionality with minimal loss of information! This makes training and image generation more efficient and allows for high-resolution image synthesis.


## Image Captioning with Transformers

## Reference

Please make sure to cite properly in your work, for example:

[1] Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016.

---
