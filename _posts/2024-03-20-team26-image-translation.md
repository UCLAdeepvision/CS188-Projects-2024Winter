---
layout: post
comments: true
title: Image Style Translation
author: Lawrence Liao, John Li, Guanhua Ji, William Wu
date: 2024-03-22
---

> This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.

<!--more-->

{: class="table-of-content"}

- TOC
  {:toc}

## Main Content

Your survey starts here. You can refer to the [source code](https://github.com/lilianweng/lil-log/tree/master/_posts) of [lil's blogs](https://lilianweng.github.io/lil-log/) for article structure ideas or Markdown syntax. We've provided a [sample post](https://ucladeepvision.github.io/CS188-Projects-2022Winter/2017/06/21/an-overview-of-deep-learning.html) from Lilian Weng and you can find the source code [here](https://raw.githubusercontent.com/UCLAdeepvision/CS188-Projects-2022Winter/main/_posts/2017-06-21-an-overview-of-deep-learning.md)

## Introduction

Our team is investigating style transfer in the context of image-to-image translation.

## Background

### Image-to-image translation

The goal of image-to-image translation is learning a mapping between input image and the output image. The dataset used for this problem is typically two sets of images we want to learn the mapping between. These datasets come in the form of

1. Paired: the dataset is tuples of image in set 1 and corresponding image in set 2
2. Unpaired: the dataset just has two sets of images without 1-to-1 correspondence.

Paired datasets are easier to train on, but they maybe hard to collect (e.g. there is not a Monet painitng for every real image). Thus, we need methods to effectively train on unpaired datasets.

## Cycle-GAN

Generative adversarial network (GAN) are deep learning frameworks that relies on a generator G and a discriminator D. Cycle GAN introduces **cycle consistency** (similar to language translation, where a sentence in English when translated to German then translated back should be the same as English).

The goal of our Cycle GAN is to learn a mapping between two image styles $X$ and $Y$. So if we have preserve cycle consistency, the ideaaaa is that our tralated image will preserve most of its semanics besides the style change.

To preserve cycle consistency, we want to make sure when our network translates an image, we can translate it back to get a similar image to the original image. In order to do this, we train two GANs together, Gan 1 $(G, D_Y)$ translating from style $X$ to style $Y$. Gan 2 $(F, D_X)$ translating from style $Y$ to style $X$. We additionally introduce a normalization term on the input image $I$ and the $F(G(I))$, the input image translated twice.

Now, in the actual style transfer process, we can use $G$ to translate from style $X$ to style $Y$, and $F$ to translate from style $Y$ to style $X$.

### Training

To train Gan 1 $(G, D_Y)$, where $G$ is a mapping from $X$ to $Y$ and $D_Y$ is a discriminator for $Y$, we use the following loss function:

$$
\mathcal{L}_{\text{GAN}}(G, D_T, X, Y) = \mathbb{E}_{y\ sim p_{\text{data}}(y)}[\log D_Y(y)] + \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log(1 - D_Y(G(x)))]
$$

We want $G(x)$ to generate images similar to the distribution of $Y$, and we want $D_Y$ to distinguish between images generated by $G$ and images from $Y$. So we wan $G$ to minimize the loss and $D_Y$ to maximize the loss.

Additionally, to preserve cycle consistency, we need a cycle consistency loss:

$$
\mathcal{L}_{\text{cyc}}(G, F) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\|F(G(x)) - x\|_1] + \mathbb{E}_{y \sim p_{\text{data}}(y)}[\|G(F(y)) - y\|_1]
$$

The idea is that for $x \in X$ and $y \in Y$, we want $F(G(x))$ to be similar to $x$ and $G(F(y))$ to be similar to $y$.

The overall obejctive function is then:

$$
\mathcal{L}(G, F, D_X, D_Y) = \mathcal{L}(G, D_Y, X, Y) + \mathcal{L}(F, D_X, Y, X) + \lambda \mathcal{L}_{\text{cyc}}(G, F)
$$

where $\lambda$ is a hyperparameter that controls the importance of the cycle consistency loss. And we hope to find $G^*, F^*$ where

$$
G^*, F^* = \arg \min_{G, F} \max_{D_X, D_Y} \mathcal{L}(G, F, D_X, D_Y)
$$

## Stable Diffusion

Stable Diffusion is in the class of latent diffusion models (LDMs). Similar to diffusion models, LDMs work by repeatedly removing noise from an image, but instead of starting in the pixel space, LDMs start in the latent space. 

Why might this be a good idea?  
We can reduce the dimensionality with minimal loss of information! This makes training and image generation more efficient and allows for high-resolution image synthesis.

### Training

[TODO]

## Image Captioning with Transformers

To caption an image, we can use a encoder-decoder architecture. To do so, we use a pre-trained Transformer-based vision model as encoder and  a pre-trained language model as decoder. The encoder produces an embedding of the image, which can be used by the decoder to generate a caption.

## Demo

### Cycle-GAN
We followed the instruction from th github repository of [Cycle-GAN paper](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix?tab=readme-ov-file) and focused on the Monet dataset in particular. We found that the training process was very slow and the results after 100 epochs were not good. We expected this to be due to the difficulty of training GAN models and the complexity of the Monet dataset. Then we downloaded the pretained model fro the authors and tested on test dataset. 

```bash
# clone the repository
git clone git@github.com:junyanz/pytorch-CycleGAN-and-pix2pix.git
cd pytorch-CycleGAN-and-pix2pix
# download the dataset
bash ./datasets/download_cyclegan_dataset.sh monet2photo
# train the model
python train.py --dataroot ./datasets/monet2photo --name monet_cyclegan --model cycle_gan
# test the model
python test.py --dataroot ./datasets/monet2photo --name maps_cyclegan --model cycle_gan
# download the pretrained model (style to monet)
bash ./scripts/download_cyclegan_model.sh style_monet
# test the pretrained model
python test.py --dataroot datasets/monet2photo/testB --name style_monet_pretrained --model test --no_dropout
```


### Stable Diffusion
We used the [CompVis sdv1.4](https://huggingface.co/CompVis/stable-diffusion-v1-4) available on Hugging Face for generation. We used the [nlpconnect/vit-gpt2-image-captioning](https://huggingface.co/nlpconnect/vit-gpt2-image-captioning) available on Hugging Face for image captioning. We first tested on the pretrained model and then fine-tuned on monet dataset.

To translate the image, we first generated the captions of the input using image (photo) captioning model, then we modified the captions to conditioned on Monet style. We then used the stable diffusion model to generate the image (painting) from the modified captions.

```python
# generate
import os
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from diffusers import StableDiffusionPipeline
from PIL import Image
import torch
import pandas as pd
from tqdm import tqdm

def generate_caption(image, feature_extractor, tokenizer, model):
    pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values
    pixel_values = pixel_values.to(device)
    output_ids = model.generate(pixel_values)
    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return caption

def generate_images(folder_path, generate_path):
    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16).to(device)
    img_caption = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning", torch_dtype=torch.float16).to(device)
    feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

    for file_name in tqdm(os.listdir(folder_path)):
        if file_name.endswith(('.png', '.jpg', '.jpeg')):
            image_path = os.path.join(folder_path, file_name)
            image = Image.open(image_path).convert('RGB')
            caption = generate_caption(image, feature_extractor, tokenizer, img_caption)
            caption = f'a painting of {caption}by Claude Monet.'
            output = pipe(caption, guidance_scale=5).images
            output[0].save(os.path.join(generate_path, file_name))

# Example usage
device = 'cuda' if torch.cuda.is_available() else 'cpu'
folder_path = './monet2photo/testB/'
# model_id = "CompVis/stable-diffusion-v1-4"
generate_path = './monet2photo/generated_monet_1000/'
model_id = './models/monet-1000/'

generate_images(folder_path, generate_path)
```

We fine-tuned the t2i stable diffusion model on Monet dataset. We first generated the captions of the Monet paintings then passed the painting, captions pairs to the pretained model for fine-tuning.

```python
# captioning
import os
from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer
from PIL import Image
import torch
import pandas as pd
from tqdm import tqdm

def generate_caption(image, feature_extractor, tokenizer, model):
    pixel_values = feature_extractor(images=image, return_tensors="pt").pixel_values
    pixel_values = pixel_values.to(device)
    output_ids = model.generate(pixel_values)
    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return caption

def process_images(folder_path, csv_file_path):
    img_caption = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning").to(device)
    feature_extractor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    tokenizer = AutoTokenizer.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
    
    data = []

    for file_name in tqdm(os.listdir(folder_path)):
        if file_name.endswith(('.png', '.jpg', '.jpeg')):
            image_path = os.path.join(folder_path, file_name)
            image = Image.open(image_path).convert('RGB')
            caption = generate_caption(image, feature_extractor, tokenizer, img_caption)
            caption = f'a painting of {caption}by Claude Monet.'
            data.append([file_name, caption])
    
    # Create a DataFrame and save to CSV
    df = pd.DataFrame(data, columns=['file_name', 'text'])
    df.to_csv(csv_file_path, index=False)

# Example usage
device = 'cuda' if torch.cuda.is_available() else 'cpu'
folder_path = './monet2photo/trainA/'
csv_file_path = './monet2photo/trainA/metadata.csv'
process_images(folder_path, csv_file_path)
```

To fine-tune t2i diffusion, we used the python code from Hugging Face [train_text_to_image.py](https://github.com/huggingface/diffusers/blob/main/examples/text_to_image/train_text_to_image.py). Here is the script:
  
```bash
export MODEL_NAME="CompVis/stable-diffusion-v1-4"
export TRAIN_DIR="./monet2photo/trainA"
export OUTPUT_DIR="./models/monet-3000"

accelerate launch train_text_to_image.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --train_data_dir=$TRAIN_DIR \
  --use_ema \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --gradient_accumulation_steps=4 \
  --gradient_checkpointing \
  --mixed_precision="fp16" \
  --max_train_steps=3000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --lr_scheduler="constant" --lr_warmup_steps=0 \
  --output_dir=${OUTPUT_DIR}
```

We fine-tuned for 1000 epochs and 3000 epochs.

## Reference

Please make sure to cite properly in your work, for example:

[1] Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." _Proceedings of the IEEE conference on computer vision and pattern recognition_. 2016.

---
