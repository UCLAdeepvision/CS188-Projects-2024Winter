---
layout: post
comments: true
title: 3D Model Generation through Machine Learning
author: Daniel Tsai
date: 2024-03-21
---


> Ever since the advent of 3D graphics, computer graphics engineers have been looking for better, more streamlined ways to create models (data representing a 3D object), 
> a historically a tedious and long process. The goal of this post is to trace the history of 3D model generation through AI vision algorithms and suggest a potential
> method not just generating 3D models but pre-preparing them for use in animation and game development.

> _To be clear we will be discussing how AI is leveraged to generate models in a format usable by the wider 3D community. We will not be discussing algorithms that choose
> to represent 3D data in a form unusable by traditional animation software such as Blender, Maya or Unity (e.g. unconverted NERF data)._


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

**Note: This post will attempt to use the word "Algorithm" in place of "Model" to refer to Machine Learning models for clarity, due to the ambiguity of whether model refers to a 3D model or an
ML model.**

## Classical 3D Modelling
3D graphics were a revolution when they were introduced. For the first time, computer were showing virtual world within the screen, not just flat images and text. As 3D graphics
have improved, the technology has equally grown in complexity. 
As of the writing of this post, 3D computing has been used in simulations, movies, animation, games, and starting to grow into Virtual and Augmented reality. As a result, the need
for high quality 3D computing has become a near necessity in current tech and engineering landscape.

One of the core parts of 3D graphics are models, data containing information about how to display a 3D object. 
The job of a 3D modeller is therefore to convert objects in the real world into 3D models such that they can be used in other applications.

![YOLO]({{ '/assets/images/team1/3dModelConversion.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. A real soccer ball versus it's counterpart as a 3D model.* [1] [2]

The issue with the manual creation of 3D models is time. While simple models may take a couple of hours to create, complex 3D objects with innings, concavities, minor details, etc. will
take onwards of weeks to get looking right.

![YOLO]({{ '/assets/images/team1/Mushrooms_by_a_tree_stump_5.jpg' | relative_url }})
{: style="width: 400px; height: 400px; overflow: hidden; max-width: 100%;"}
*Fig 2. A highly detailed picture of a tree stump in an overgrown forest.* [3]

**Thus the question is, how can we leverage AI to speed up the process of modelling?**

## Input and Output Formats

### Inputs

As the job of a 3D modeller is to convert real objects to 3D models, the goal of a 3D ML model is to convert an image or multiple images of an object
into a 3D model. Whether we use a single image or multiple images will significantly impact how our model works and the quality of the models generated,
as such it is important to consider the pros and cons of each.

#### Multi-Image

Multi-image models refers to the fact that the algorithm takes in multiple images of a single object from many different perspectives. The benefits of this
is obvious, with more data of the object the algorithm has access to the more accurate of a 3D model it can generate. The core issue with this method is obtaining
such a dataset, for large image sets of object isn't hard to obtain, but large image sets of the exact same object are more difficult to find. To add on, multi-image data sets
are also inherently less flexible for end-users, requiring them to also obtain multi-perspective images of their object while also requiring any other information
the model may require of camera position data.

The most common method to generate such datasets is to take virtual snapshots of 3D models, which are significantly easier to obtain.

![YOLO]({{ '/assets/images/team1/Nerf.png' | relative_url }})
{: style="width: 600px; overflow: hidden; max-width: 100%;"}
*Fig 3. A series of images from the NeRF dataset by Ben Mildenhall, Pratul P. Srinivasan, Mathew Tancik et al.* [4]

#### Single Image

With single image algorithms, the goal is to generate a complete 3D model only using a single object in question. The immediate benefit for both the ML developer and
end-user is that it is extremely simple to obtain an image of an object, as there are hundreds of millions of images on the internet. The issue for the algorithm is that 
it has much less data to work with, and must predict what the object's obfuscated structure is from an understanding of spatial data and the typical structure of objects
within a category.

![YOLO]({{ '/assets/images/team1/cifar10.png' | relative_url }})
{: style="width: 400px; overflow: hidden; max-width: 100%;"}
*Fig 4. A series of images from the CIFAR-10 dataset.* [5]

### Outputs

There are many different ways models can be stored, this
post will focus on three commonly used formats by animators and game designers, **point clouds**, **voxels**, and **meshes**.

It should be noted that regardless of the initial format, the _end-goal_ of all these methods is to generate a mesh. We won't note the conversion
process between these formats extensively as the conversion process is already well-defined by external, non-AI related tools, _unless_ it is related to
the algorithm being discussed.

#### Voxels

In this format, model are defined by voxels is just a collection of 3D points in XYZ space snapped to a three-dimensional grid of fixed size. These
are typically the easiest to store (as they're just lists of three numbers) and the easiest to comprehend, but doesn't yield the best results. Due to the 
grid like nature of how voxels are defined, it is a trade-off between a smooth, detailed model or a space-efficient model storage-wise. However, voxel are
a decent precursor to a mesh creation due to its well-defined volumes. In terms of rendering, voxels simply fill in cubes where points are defined, giving the overall
model a boxy shape. While not ideal, 3D modellers are able to work with this format relatively easily by "filling in cubes", and given certain aesthetics the
"voxelated" look may even be desired. 

![YOLO]({{ '/assets/images/team1/voxel_art.png' | relative_url }})
{: style="width: 600px; overflow: hidden; max-width: 100%;"}
*Fig 5. Voxel art by user sebrein on DeviantArt.* [6]

#### Point-Cloud

Point clouds are similar to voxels in the way that they're also defined as a collection of points in XYZ space. However, the points in a point cloud aren't restricted
to any set grid, and therefore can exist in any place in 3D space. The trade-off for this level of detail is that point clouds don't have a simple method
to render them without some sort of conversion or lost of accuracy. For example, point clouds may be first converted to a voxel representation via rounding, then
rendered that way. Another is to send a high density of ray-casts out from the rendering camera and approximating which points each ray would
hit to a certain threshold. Overall though, point clouds are unideal to work with as a 3D modeller for it's extremely hard to visualize what a point
cloud would look like post-rendering, and it is highly unclear how an artist would "edit" a point cloud. The prevalence of point clouds is mainly 
due to the fact that it's the best format for live capture of real-world 3D data, and as such there are many, well-developed tools that can do the
conversion of point-clouds for us.

![YOLO]({{ '/assets/images/team1/Point_cloud_torus.gif' | relative_url }})
{: style="width: 300px; overflow: hidden; max-width: 100%;"}
*Fig 6. A point cloud representation of a torus.* [7]

#### Mesh
Meshes are the most common way 3D models are stored as their definition is easy to understand while giving 3D artists the greatest amount of
control over the shape of a model. They are defined by three different aspects, _points_, _edges_, and _faces_. 

Points are the simplest concept to grasp, they're a list of discrete points that define the vertices of a 3D
object. The most important data associated with points are their coordinates in XYZ space, for this data is absolutely
required for the definition of a mesh's shape. However, additional data may also be stored per point. For example, data associated
with the normal of each point may be stored, which is later used to render (displaying 3D data) the object. Normals define which direction
is considered "pointing" outwards at a specific point, ensuring that the model isn't inside out and additionally can be used with
interpolation to smooth the appearance of meshes without the need for more points.

Edges are edges of the 3D model, the exact same edges of a 3D shape. They're defined as pairs of points, and typically don't have any
additional data associated with them other than the two point that define them.

Faces are the visible parts of a mesh, and the parts that are actually displayed when rendering a 3D model. They are defined by a list of points,
and the normal of a face is defined as the cross product of the vectors between the points. In most meshes, faces are triangles as it significantly
simplifies the calculation of face normals, defining the two required vectors for the cross product as a vector from point 1 to
point 2 and a vector from point 1 to point 3. Because of this, the points that define the face must be a counterclockwise order, thus meaning
that the ordering of points is extremely important.

![YOLO]({{ '/assets/images/team1/Dolphin_triangle_mesh.png' | relative_url }})
{: style="width: 500px; overflow: hidden; max-width: 100%;"}
*Fig 7. A 3D mesh model of a dolphin.* [7]

## Aside

To assist with testing the different algorithms in accuracy and creativity, I introduce JC.

![YOLO]({{ '/assets/images/team1/JC.png' | relative_url }})
{: style="width: 500px; overflow: hidden; max-width: 100%;"}
*Fig 7. JC, our robot companion.*

JC is a simple 3D model I made in blender out of a simple sphere. Their purpose is to test how accurately the following algorithms can
recreate a given model, and how creative an algorithm is if they are single-image algorithms. JC will also serve as proof that these algorithms
were run by me. Their obj file is linked below:

[Download JC](../../../assets/images/team1/JC.obj)

## Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction

The following will be a look into a point cloud generation, single-image method developed by Chen-Hsuan Lin, Chen Kong, and Simon Lucey [9].
Their algorithm bucked the trend of 3D ConvNets that generated data in voxel grids. Instead, they sought to use point clouds, 
a route with the potential to run much faster and at better resolution by solely using 2D CNNs rather than expensive 3D CNNs.

### Methodology

![YOLO]({{ '/assets/images/team1/point_cloud_network.png' | relative_url }})
{: style="width: 500px; overflow: hidden; max-width: 100%;"}
*Fig 8. The network architecture of the Point Cloud Generation Network by Lin et al.* [9]

There are two major portions of the network, a structure generator that generates the point cloud, and a pseudo-renderer that generates images
involved in the loss-calculation of the algorithm.

#### Structure Generator

The structure generator follows a standard generative model architecture with opposing encoder and generator convolutional networks. The encoder first
converts the input image (an RGB image) into a latent space, then attempts to generate N different depth maps from that latent space. The N different
depth maps are representative what a depth map would from N different camera positions and angles. It can be visualized as a generative network that generates
images of the input from a series of predefined perspectives, except that instead of each pixel having an RGB value, the pixels only have a single value
called depth representing its distance from the camera, therefore representing XYZ coordinates.

These depth maps are then transformed via the known camera transformation matrices into global XYZ space, then combined into a single point cloud, the desired
output of the algorithm.

![YOLO]({{ '/assets/images/team1/generated_point_cloud.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 9. A cropped example from the paper showing an input image and the generated point cloud by the algorithm.* [9].

#### Pseudo-Renderer and Optimization

To evaluate the quality of the generated point cloud, the algorithm evaluates the loss between a projection of the point cloud at a fixed perspective versus a 
project from the same perspective of the ground truth CAD model (a specific version of a mesh 3D model). This is done as opposed to using a 3D metric (e.g. 3D distance
between a 3D and a point cloud), such methods run into the issue of being too computationally expensive and therefore slow. Projections on the other hand can be compared
relatively easily via traditional, 2D image loss functions. For this algorithm, the authors chose to use two loss functions, one for mask(\ref{eq:mask_loss}) (binary cross-entropy loss)
and one for depth(\ref{eq:depth_loss}) (L1 loss) to optimize their algorithm. 

$$
\begin{align}
\tag{1.1}

\mathcal{L}_{\text{mask}} = \sum_{k= 1}^K -M_k \log \hat M_k - (1 - M_k) \log \left(1 - \hat M_k \right)
\label{eq:mask_loss}

\end{align}
$$

$$
\begin{align}
\tag{1.2}

\mathcal{L}_{\text{depth}} = \sum_{k = 1}^K \left\Vert \hat Z_k - Z_k \right \Vert_1
\label{eq:depth_loss}

\end{align}
$$

$$
\begin{align}
\tag{1.3}

\mathcal{L} = \mathcal{L}_{\text{mask}} + \lambda \cdot \mathcal{L}_{\text{depth}}

\end{align}
$$

To generate the projection for the point cloud, a pseudo-renderer in place of a full renderer. 
The point cloud is first transformed into the transform space of a selected perspective, the projected to the XY plane.
The difficulty comes with conflicting points that attempt to project onto the same XY coordinates. So solve this, the rendering target space is temporarily up-sampled
to a higher resolution to reduce conflicts, then a max-pooling operation on the _inverse depth values_ (z) to down-sample the image back
to the intended resolution, thereby retaining the depth values _closest_ to the camera. Thus the algorithm can generate a projection
while maintaining the ability to back propagate loss throughout the entire network (something a traditional renderer would be unable to do).

![YOLO]({{ '/assets/images/team1/pseudo_renderer.png' | relative_url }})
{: style="width: 700px; max-width: 100%;"}
*Fig 10. A visual representation of the pseudo-renderer and the effects of different up-sampling factors.* [9].

### Authors' Results

This post won't dive into every experiment ran in the paper in thorough detail. We will instead highlight the successes and potential applications of the
developed algorithm.

#### Single Category

For single-category experiments (i.e. training on objects only of the same category, in this case chair), their algorithm was able
to generate point clouds of much higher density and finer granularity than other algorithms of the time. Their takeaway from these experiments
was that the explicit encoding of 3D spatial data (the pre-specified camera angles) significantly improves the efficiency and
efficacy of the algorithm compared to hoping that the algorithm implicitly learned structure.

![YOLO]({{ '/assets/images/team1/point_cloud_single.png' | relative_url }})
{: style="width: 700px; max-width: 100%;"}
*Fig 11. Single category results of Lin et al. algorithm compared to other algorithms.* [9].

Visually inspecting the results of their models, it is very clear that the higher granularity significantly benefits the output of the models.
Details such as the spokes of the chair are typically lost in the prior algorithms, while the author's algorithm modelled them without any
significant loss.

#### General Category

For general category results their algorithm similarly generated much denser results than any previous algorithm, though it is also clear
the fine granularity of the algorithm was lost. For thin structures, the model had a difficult time filling the areas in with a high enough
density and finer details seemed to be lost entirely. However, overall it does preform better at these tests than competing algorithms at the time.

![YOLO]({{ '/assets/images/team1/point_cloud_multiple.png' | relative_url }})
{: style="width: 700px; max-width: 100%;"}
*Fig 12. General category results of Lin et al. algorithm compared to other algorithms.* [9].

## Our Experiments

In the following section, we'll be testing the algorithm with JC. Unfortunately the author's only provide the single-category algorithm,
as such we'll only be testing on chairs. For testing with JC, we will be modifying JC to be more chair-like.

### Image
Please create a folder with the name of your team id under /assets/images/, put all your images into the folder and reference the images in your main content.

You can add an image to your survey like this:
![YOLO]({{ '/assets/images/UCLAdeepvision/object_detection.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. YOLO: An object detection method in computer vision* [1].

Please cite the image if it is taken from other people's work.


### Table
Here is an example for creating tables, including alignment syntax.

|             | column 1    |  column 2     |
| :---        |    :----:   |          ---: |
| row1        | Text        | Text          |
| row2        | Text        | Text          |



### Code Block
```
# This is a sample code block
import torch
print (torch.__version__)
```


### Formula
Please use latex to generate formulas, such as:



or you can write in-text formula $$y = wx + b$$.

### More Markdown Syntax
You can find more Markdown syntax at [this page](https://www.markdownguide.org/basic-syntax/).

## References

[1] ["Soccer Ball"](https://sketchfab.com/3d-models/soccer-ball-88590cf1e42e44bfb85ce3b6b1959648) by [tinmanjuggernaut](https://sketchfab.com/tinmanjuggernaut) is licensed under [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)
[2] ["Giant Soccer Ball"](https://upload.wikimedia.org/wikipedia/commons/c/c3/Giant_Soccer_Ball.jpg) by A. Scott Fulkerson / Circle Jerk Productions (TM) is licensed under [CC BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/)
[3] ["Mushrooms by a tree stump 5"](https://upload.wikimedia.org/wikipedia/commons/d/d2/Mushrooms_by_a_tree_stump_5.jpg) by [W.carter](https://commons.wikimedia.org/wiki/User:W.carter) is in the [Public Domain, CC0](https://wiki.creativecommons.org/wiki/Public_domain)
[4] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, [“NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis.”](https://arxiv.org/abs/2003.08934) arXiv, 2020. doi: 10.48550/ARXIV.2003.08934.
[5] Krizhevsky, A., Nair, V. and Hinton, G, "The CIFAR-10 Dataset" University of Toronto, 2014. https://www.cs.toronto.edu/~kriz/cifar.html
[6] ["Farming island isometric voxel art"](https://www.deviantart.com/sebrein/art/Farming-island-isometric-voxel-art-590754260) by [sebrein](https://www.deviantart.com/sebrein/gallery) is licensed under [CC BY-ND 3.0](https://creativecommons.org/licenses/by-nd/3.0/)
[7] ["Point cloud torus"](https://upload.wikimedia.org/wikipedia/commons/4/4c/Point_cloud_torus.gif) by [Lucas Vieira](https://commons.wikimedia.org/wiki/User:LucasVB) is in the [Public Domain, CC0](https://wiki.creativecommons.org/wiki/Public_domain)
[8] ["Dolphin triangle mesh"](https://upload.wikimedia.org/wikipedia/commons/f/fb/Dolphin_triangle_mesh.png) by [Chrschn](https://en.wikipedia.org/wiki/User:Chrschn) is in the [Public Domain, CC0](https://wiki.creativecommons.org/wiki/Public_domain)
[9] C.-H. Lin, C. Kong, and S. Lucey, “Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction.” arXiv, 2017. doi: 10.48550/ARXIV.1706.07036

---
