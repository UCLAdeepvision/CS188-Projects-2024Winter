---
layout: post
comments: true
title: Biases in Computer Vision
author: Team15
date: 2024-03-15
---


> In this blog post, we investigate the issue of biases and model leakage in computer vision, specifically in classification tasks. We discuss traditional and deep learning approaches to prevent biases in classification models. 


<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
Trained computer vision classifier models reflects our own biases in society. For example, google baseball players and you will get pictures of almost all males. If you train a generative model to draw a person cooking in a home kitchen from Internet pictures, you are more likely to get a woman than a man. On the surface, these results come from unbalanced traning data. But the issue goes deeper than that. In the next few sections, we will discuss dataset leakage and model leakage, which result in biased behaviors of our models, classical approaches to correct biases, and a novel deep learning approach. 

We base our discussion on two academic papers. We will discuss a few classical approaches from Mehrabi et al. We will discuss the adversarial debiasing method from Wang et al. 

## Dataset Leakage and Model Leakage
To investigate the biases of our models, we need to first define two leakages: dataset leakage and model leakage. Dataset leakage measures the predictability of protected information, i.e. gender, from ground truth labels. Model leakage measures the predictability of protected information from model prediction. 

A model is said to be amplifying biases if it exhibits stronger model leakage than dataset leakage. 

![Kitchen]({{ '/assets/images/15/kitchen.jpg' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. Two women in a kitchen setting. Imbalanced datasets display strong dataset leakages.* [1].

Now, let's take a closer look at these two leakages. 

Dataset leakage is defined as the the ability of an attacker f to successfully guess the protected information g based on the ground truth label Y. A dataset with, for example, overwhelming more female cooks than male cooks, can have a high level of dataset leakage. 

$$
\lambda_D = \frac{1}{|D|} \sum_{(Y_i,g_i) \in D} 1\{f(Y_i) == g_i\}
$$

Model leakage is similarly defined, except that the attacker f is guessing the protected information g based on the model prediction instead of the grounth truth label. 

$$
\lambda_M = \frac{1}{|D|} \sum_{(\hat{Y}_i,g_i) \in D} 1\{f(\hat{Y}_i) == g_i\}
$$

And finally, we define bias amplification to be the difference between model leakage and dataset leakage. It is to be expected for a model trained on imbalanced data to display large model leakage. What is even more concerning is that models tend to amplify biases, even when trained on balanced datasets. 

$$
\Delta = \lambda_M - \lambda_D
$$

## Classical Approach #1

## Classical Approach #2

## Challenges of Classical Approaches

## Deep Learning Approach: Adversarial Debiasing


### Table
Here is an example for creating tables, including alignment syntax.

|             | column 1    |  column 2     |
| :---        |    :----:   |          ---: |
| row1        | Text        | Text          |
| row2        | Text        | Text          |



### Code Block
```
# This is a sample code block
import torch
print (torch.__version__)
```


### Formula
Please use latex to generate formulas, such as:

$$
\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}
$$

or you can write in-text formula $$y = wx + b$$.

### More Markdown Syntax
You can find more Markdown syntax at [this page](https://www.markdownguide.org/basic-syntax/).

## Reference
Please make sure to cite properly in your work, for example:

[1] Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2016.

---
