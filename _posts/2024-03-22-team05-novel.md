---
layout: post
comments: true
title: Novel View Synthesis
author: Joe Lin, Michael Song, and Alexander Chien
date: 2024-03-22
---

> This block is a brief introduction of your project. You can put your abstract here or any headers you want the readers to know.

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
The task of novel view synthesis involves generating new scene views given a limited set of known scene perspectives. Solving this problem can open the doors to recinematography, refined virtual reality, quality scene representations, and more. Successes in this area have primarily sprouted from recent machine learning approaches, namely neural radiance fields (NeRFs) and 3D gaussian splatting (3DGS).

In this article, we give an overview of both a classical and deep learning approach and discuss their pitfalls in order to motivate 3D gaussian splatting, an approach we will investigate in detail.

## A Classical Approach: Light Fields
One of the most prevalent classical methods for novel view synthesis is by representing a scene with a light fields. These light fields are 4D representations of a scene's radiance as a function of the light ray's position and direction. 

Light Slabs are defined by the function: $$L(u, v, s, t)$$, where $$u, v, s, t$$ are values that describe two coordinate systems. The idea is that given two 2D planes in space, a ray in the scene will intersect each plane at exactly one point. By defining the first plane with coordinates $$(u, v)$$ and the second plane with $$(s, t)$$, we can uniquely describe a ray with these four coordinates. 

To learn all the function $$L(u, v, s,t)$$, we make use of the many different angles captured of a scene. Each view will then be contain many rays described by the $u, v$ and $s, t$ coordinates, and the corresponding radiance will be mapped. Then, to synthesis a new view, a novel 2D slice can be taken. By placing a new camera in the scene, rays can be projected from the scene onto to this camera coordinate system, which is denoted as a set of Rays $${R_1, R_2, ..., R_N}$$. Each ray $$R_i$$ is defined by the coordinates $$(u_i, v_i, s_i, t_i)$$ and the radiance is mapped to the pixel.

There are two main downsides to this approach. First, to create an accurate representation of the field, we need many unique views of a scene. If there a lack of viewing angles, the learned function $$L(u, v, s, t)$$ may be inaccurate, and result in poor syntheses. Second, the light fields method struggles with scenes that involve occlusion. This type of scene tends to be challenging, and recent research have explored methods to improve performance of Light Field based view synthesis involving deep learning methods. For the rest of this article, we focus on two different novel view synthesis solutions.

## Neural Radiance Fields (NeRFs)
On a high level, NeRFs aim to optimize a **continuous volumetric scene function** $$F_\theta$$ that essentially implicitly encodes a scene representation. More formally, the input of $$F_\theta$$ is a spatial location $$(x, y, z)$$ and viewing direction $$(\theta, \phi)$$ and the output is the predicted volume density $$\sigma$$ and view-dependent emitted radiance.

### NeRF Algorithm Overview
1. Generate rays from known **camera poses** and collect **query points**.
2. Train 3D scene representation with MLP.
    - Input location and viewing direction of query points.
    - Predict color and volume density.
    - Render based on each ray's volume density profile.
    - Compute loss between rendered image and ground truth image.

### Generating Rays and Query Points
Given a collection of camera poses, we want to first generate a set of rays originating from the camera center and intersecting each pixel of the image plane.

We can denote each camera pose as $$v_o = (x_c, y_c, z_c)$$, where $$x_c, y_c, z_c$$ encapsulates the camera center in world coordinates. We can also represent each ray as a normalized vector $v_d$. Now, we can construct a parametric equation $$P = v_0 + t \cdot v_d$$, which defines points along our ray from a certain camera view.

The next step is to determine the set of query points for training. There exists many approaches for this subtask with the simplest being **uniform sampling**, in which we simply take $n$ equally-spaced points along the ray. Immediately, one may realize the complexities involved in constructing such a ray sampler. We want to be able to not only sample across the relevant object space within a scene but also sample more frequently in areas of higher volume density. In light of these considerations, the original authors introduce **hierarchical volume sampling**, which allows for adaptive sampling based on high-density areas.

### Network Architecture
At this point, we have obtained a set of query points (locations + viewing directions), which we can input into a multi-layer perceptron (i.e. fully connected network) and predict the color and volume density.

### Positional Encoding


## 3D Gaussian Splatting (3D-GS)
Before diving into the details of this approach, we first need to discuss core computer graphics concepts that are essential to this technique.

### Rasterization
**Rasterization** is the process of drawing graphical data onto a rasterized (pixelized) screen. In computer graphics, objects are commonly represented by simple polygonal faces like triangles. Each of these polygons are decomposed into pixels and rasterized into a raster image.

Gaussian splatting is essentially a rasterization technique. Instead of rasterizing simple polygons, we rasterize 3D gaussian distributions. Each of these gaussians can be described by its position (mean, $\mu$), splat size (covariance matrix, $\Sigma$), color (spherical harmonics), and opacity ($\alpha$). Mathematically, it can be expressed as follows:

$$G(x) = \frac{1}{2 \pi \lvert \Sigma \rvert^{\frac{1}{2}}} e^{-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu)}$$

![gaussian](https://drive.google.com/uc?export=view&id=1lp3AnXiMRPqZBHl7nUWwBkIcHYMQ0Dp-)

So, in other words, we can learn an efficient representation of a 3D scene as a composition of thousands of 3D gaussian distributions. How does this process work? Let's take a look!

### 3D-GS Algorithm Overview
1. Given a set of images, estimate a pointcloud with Structure from Motion (SfM).
2. Initialize isotropic gaussians at each point.
3. Train 3D scene representation.
    - Rasterize gaussians with a differentiable rasterizer.
    - Compute loss between raster image and ground truth image.
    - Adjust gaussian parameters with SGD.
    - Densify or prune Gaussians.

### Pointcloud Estimation
Our first objective is to determine a rough scene structure. This is accomplished by using **Structure from Motion**, a method based on **stereoscopic photogrammetry**, to construct a pointcloud.

The algorithm at its core works by detecting and tracking keypoints across a sequence of scene view images. Then, camera poses are estimated and 3D positions of keypoints are triangulated.

### Initialization of Gaussians
After pointcloud generation, we initialize an **isotropic gaussian** at every point. Isotropic means that these gaussians have diagonal covariance matrices. The mean of the gaussians are also set to the average of the distances to the 3-nearest neighbors. This initialization approach is both intuitive and performs well empirically.

### Differentiable Rasterizer
The training process begins with processing the gaussians into a **differentiable rasterizer**.

1. Project each Gaussian into 2D camera perspective.
2. Sort Gaussians by order of depth.
3. Perform front-to-back blending of Gaussians at each pixel.


#### Projection
In any rasterization procedure, we must take our objects that lie within the 3D world space and project them into the 2D image space. For the differentiable rasterizer, we would like to compute the mean $\mu$ and covariance $\Sigma$ of every 3D gaussian projected into 2D.

Before we derive these computations, we first define the **instrinsic camera matrix** $K$ and the **extrinsic camera matrix** $W$ as follows:
$$K = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix}$$
$$W = \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix}$$
where $f_x, f_y$ are the focal lengths and $c_x, c_y$ describe the camera center.

We can then compute the 2D projection of our homogeneous gaussian mean $\vec{\mu}$.
$$\begin{bmatrix} u \\ v \\ z \end{bmatrix} = K W \vec{\mu}$$
$$\begin{bmatrix} u \\ v \\ z \end{bmatrix} = \begin{bmatrix} f_x & 0 & c_x \\ 0 & f_y & c_y \\ 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} r_{11} & r_{12} & r_{13} & t_x \\ r_{21} & r_{22} & r_{23} & t_y \\ r_{31} & r_{32} & r_{33} & t_z \end{bmatrix} \begin{bmatrix} \mu_x \\ \mu_y \\ \mu_z \\ 1 \end{bmatrix}$$

To get the projected mean $\vec{\mu}^{2D}$, we need to perspective divide to obtain the 2D coordinates.
$$\vec{\mu}^{2D} = \begin{bmatrix} u / z \\ v / z \end{bmatrix}$$

Now, we want to compute the 2D projection of our covariance matrix $\Sigma$. It was straight-forward to compute the projected gaussian mean. Covariance, on the other hand, will require some further intuition.

The perspective projection is a non-linear transformation, which when means that the 3D gaussians are no longer true gaussians in the 2D projection. So, we would like to approximate the projection by linearization such that we obtain true gaussians in the projection space.

This linear approximation can be done using the Jacobian of the projection matrix, denoted as $J$. It follows that the covariances in camera coordinates are $\Sigma^{2D} = J W \Sigma W^T J^T$, where $W$ encapsulates the world to camera transformation.

#### Rendering
Once projection is complete, we now have a set of 2D gaussians (represented by the projected means and covariances). So, it remains to compose the final rendered image using an image formation model, which turns out to be quite similar to the one used in NeRFs.

Recall that in a NeRF, the image formation model used is commonly formulated as:
$$C(p) = \sum_{i = 1}^N c_i \alpha_i \prod_{j = 1}^{i - 1} (1 - \alpha_j)$$

For gaussian splatting, we have:
$$C(p) = \sum_{i = 1}^N c_i f_i^{2D}(p) \prod_{j = 1}^{i - 1} (1 - f_j^{2D}(p))$$

Despite the resemblance of their equations, the difference in computing $\alpha_i$ and $f_i^{2D}(p)$ are actually at the core of gaussian splat's stronger performance. In NeRFs, we have to query a MLP at every point to determine the volume density and emitted radiance, which will essentially give us $\alpha_i$. On the other hand, in guassian splatting, we're able to precompute all projected gaussians and directly use them at every pixel (i.e. we avoid unnecessary reprojection of gaussians).

#### Rasterizer Optimizations

An important advantage of gaussian splatting is its fast rendering time when compared to alternative deep learning methods such as NeRF. One of the largest reasons for this is the fast differentiable rasterizer. The main goals with this rasterizer is to have fast overall rendering and sorting to allow for approximate alpha-blending and avoid hard limits on the number of splats that can be learned. The key steps of the rasterizer are listed here below:
1. split screen in to 16x16 tiles
2. cull 3D Gaussians with frustum culling
3. initiate Gaussians and assign each a key based on view depth and tile ID
4. use GPU radix sort to order Gaussians
5. initialize a list per-tile with the depth ordered Gaussians that splat to that tile
6. rasterize each tile in parallel




### Optimizing Gaussians
An important property we want to keep in mind is that any covariance matrix $\Sigma$ must always be positive semi-definite. To be positive semi-definite, all of a matrix's principal minors must be positive as well. Since entries on the diagonal of $\Sigma$ (our variances) can be considered 1st order principal minors, they must therefore always remain positive. Keep in mind that a negative variance (or spread) would not have any physical meaning.

When optimizing our Gaussians, an initial thought might be to optimize $\Sigma$ directly. However, when optimizing $\Sigma$ during gradient descent, we cannot guarantee the model will maintain positive values for our variances. Instead, another approach would be to perform an eigenvalue decomposition and represent $\Sigma$ as the following:
$$\Sigma = RSS^TR^T$$

where $R$ is a rotation matrix and $S$ is a diagonal, scaling matrix (containing the standard deviations or the square-roots of our variances). The intuition behind this decomposition is to ensure that regardless of S containing positive or negative values, our variances will remain positive once we compute $SS^T$.



### Controlling Gaussians
**_Potential Issue #1:_**  
- Areas with missing geometric features(under-reconstruction).
- Regions where Gaussians cover large areas in the scene (over-reconstruction).

_Both scenarios result in large view-space positional gradients._  
- _**Intuition:** When there is a lack of Gaussians, Gaussians in the neighborhood will want to move toward the under-reconstructed area to improve coverage. When there is a large Gaussian, nearby Gaussians will want to move away from the over-reconstructed region._ 

**_Solution: Densification_**
- **Small Gaussians in under-constructed regions:** Clone the small Gaussian at the same location and move its clone in the direction of
the positional gradient.
    - Increases both the total volume of the system and the number of Gaussians.
- **Large Gaussians in regions of high variance:** Replace the large Gaussian with 2 new ones and divide
their scale by a factor of 𝜑 = 1.6 (value determined experimentally). Initialize their positions using the original, large Gaussian as a probability density function for sampling. 
    - Conserves the total volume of the system but increases the number of Gaussians.

**_Potential Issue #2:_**
- Floaters that appear close to the input cameras could cause an unjustified increase in Gaussian density.
    - **_Intuition:_** Floaters could be perceived as over-constructed regions, which the aforementioned algorithm adjusts for through densification.

**_Solution: Pruning_**
- Set $\alpha$ (opacity) close to 0 every 3,000 iterations, effectively pruning the Gaussians.

#### Additional Optimizations:

